//workDir =    "/lustre/scratch118/humgen/hgi/users/ad7/nextflow_tower/birth_cohort_wes/work"
//tmpDir =     "/lustre/scratch118/humgen/hgi/users/ad7/nextflow_tower/birth_cohort_wes/tmp"
//homeDir = "/lustre/scratch118/humgen/hgi/users/ad7/nextflow_tower/birth_cohort_wes"


params {
    // specify input bam (or cram) files in tab-delimited input file:
    // 3 columns required: sample, object (path to cram or bam file) and object_index (index .crai or .bai)
    // e.g:
    // sample   object  object_index
    //PD42171b  /lustre/path_to/PD42171b.sample.dupmarked.bam   /lustre/path_to/PD42171b.sample.dupmarked.bam.bai
    //tsv_file = "/lustre/scratch118/humgen/hgi/users/ad7/nextflow_tower/birth_cohort_wes/input_bams.tsv"
    //tsv_file = "/lustre/scratch119/humgen/projects/birth_cohort_wes/alspac/external_cram/all_samples.tsv"
    //tsv_file = "/lustre/scratch118/humgen/hgi/users/ad7/nextflow_ci_pipelines/bristol_crams/test_crams/samples.tsv"

    // how many samples from input tsv file to process
    // -1 for all samples
    samples_to_process = 5

    ref_dir = "/lustre/scratch118/humgen/resources/ref/Homo_sapiens/HS38DH"
    // path to genome ref file, relative to ref_dir/
    ref_file = "hs38DH.fa"

    bed_dir = "/lustre/scratch118/humgen/resources/exome/Homo_sapiens"
    // path to bed files, relative to bed_dir/
    bed_file_deepvariant = "Twist/Twist_Human_Core_Exome_BI-CTR_padded_merged.bed"
    bed_file_gatk = "Twist/Twist_Human_Core_Exome_BI-CTR_padded_merged.interval_list"

    // Report Directory, where pipeline run reports end up
    reportdir = "/lustre/scratch123/hgi/teams/hgi/re3/nf_reports"

    // must be one of
    // "remap_inputs" "sort_inputs" "no_remap_no_sort"
    //run_mode = "sort_inputs"

    // which downstream tasks to run:
    //run_sort_cram = true
    //remap = false
    //run_markDuplicates = true
    //run_coord_sort_cram = true
    //run_deepvariant = true
    //run_haplotypecaller = true

    // how to stage output files from work dir into results dir
    // choose "rellink", "symlink", "move" or "copy"
    copy_mode = "copy"

    // Directory to copy the GATK gVCF file to, on completion
    // haplotypecaller_output_dir = "/lustre/scratch119/humgen/projects/birth_cohort_wes/alspac/gatk"
    //  haplotypecaller_output_dir = "/lustre/scratch119/realdata/mdt3/projects/birth_cohort_wes/alspac/repeat_seq/gatk"
    //    haplotypecaller_output_dir = "/lustre/scratch123/hgi/teams/hgi/re3/nf_ddd_africa_test/gatk"

    // Directory to copy DeepVariant gVCF and VCF files to on completion
    // deepvariant_output_dir = "/lustre/scratch119/humgen/projects/birth_cohort_wes/alspac/dv_gln"
    //  deepvariant_output_dir = "/lustre/scratch119/realdata/mdt3/projects/birth_cohort_wes/alspac/repeat_seq/dv_gln"
    //  deepvariant_output_dir = "/lustre/scratch123/hgi/teams/hgi/re3/nf_ddd_africa_test/dv_gln"

    // Directory to copy CRAM file to after pre-calling processing
    // cram_output_dir = "/lustre/scratch119/humgen/projects/birth_cohort_wes/alspac/sanger_cram"
    // cram_output_dir = "/lustre/scratch119/realdata/mdt3/projects/birth_cohort_wes/alspac/repeat_seq/sanger_cram/"
    //   cram_output_dir = "/lustre/scratch123/hgi/teams/hgi/re3/nf_ddd_africa_test/cram"

     //run_mode = "study_id"
     //run_mode = "tsv_file"

     study_id_mode {
          run_imeta_study = false // whether to run task to list all samples and cram from study ids
          run_imeta_study_lanes = false // whether to run task to list all samples and cram from study ids restricted to specific run_ids
          input_studies = "6595" // list of study_ids to pull, can be more than one (seperated by commas).
          exclude_samples_file = "/lustre/scratch119/humgen/projects/birth_cohort_wes/mcs/sample.list" // file containing a list of files that we do NOT want to process
          //input_study_lanes = "41864,42016" // list of run_ids to pull from study, can be more than one (seperated by commas).
     }

    // input parameters common to all input modes: 
    run_imeta_samples = false // whether to run task to list all samples and cram from provided list of samples IDs from csv input table (created from google Spreadsheet).
    run_iget_study_cram = true // whether to run task to iget all samples cram files



    // the following are for one-off tasks run after workflow completion to clean-up work dir:
    on_complete_uncache_irods_search = false // will remove work dir (effectively un-caching) of Irods search tasks that need to be rerun on next NF run even if completed successfully.
    on_complete_remove_workdir_failed_tasks = false // will remove work dirs of failed tasks (.exitcode file not 0)
    // TODO: on_complete_remove_workdir_notsymlinked_in_results = false // will remove work dirs of tasks that are not symlinked anywhere in the results dir. This might uncache tasks.. use carefully..


}

report {
  enabled = true
  file = "${params.reportdir}/report.html"
}


timeline {
  enabled = true
  file = "${params.reportdir}/timeline.html"
}

trace {
  enabled = true
  file = "${params.reportdir}/trace.txt"
}
process {
  
  withName: iget_study_cram {
    maxRetries = 3
    memory = '3G'
    maxForks = 6
    time = '120m'
    cpus = 1
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
  }

  withName: imeta_study {
    maxRetries = 3
    maxForks = 12
    memory = '8G'
    cpus = 1
    time = '240m'
    errorStrategy = { task.attempt <= 1 ? 'retry' : 'ignore' }
  }

  withName: bam_to_cram {
    container  = 'file:///software/hgi/containers/samtools-1.10.sif'
    containerOptions = "--bind /lustre --bind ${params.ref_dir}:/ref --bind /tmp:/tmp"
    //clusterOptions = { "-M 18000 -R \"select[mem>=18000] rusage[mem=18000]\" -R \"select[model==Intel_Platinum]\"" }
    //publishDir "${params.cram_output_dir}", mode: 'copy', overwrite: true, pattern: "*cram*"
    maxRetries = 3
    memory = '8G'
    cpus = 1
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    time '300m'
    queue 'normal'
  }

  withName: coord_sort_cram {
    container  = 'file:///software/hgi/containers/sambamba_0.6.4.sif'
    containerOptions = "--bind /lustre --bind ${params.ref_dir}:/ref --bind /tmp:/tmp"
    //clusterOptions = { "-M 18000 -R \"select[mem>=18000] rusage[mem=18000]\" -R \"select[model==Intel_Platinum]\"" }
    maxRetries = 3
    memory = '18G'
    cpus = 1
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    time '300m'
    queue 'normal'
  }

  withName: deepvariant {
    container  = 'file:///software/hgi/containers/deepvariant_0.10_UKB.sif'
    containerOptions = "--bind /lustre --bind ${params.ref_dir}:/ref --bind ${params.bed_dir}:/bed_files --bind /tmp:/tmp"
    //clusterOptions = { "-M 8000 -R \"select[mem>=8000] rusage[mem=8000]\" -R \"select[model==Intel_Platinum]\"" }
    //publishDir "${params.deepvariant_output_dir}/gvcf", mode: 'copy', overwrite: true, pattern: "*dv.g.vcf.gz*"
    //publishDir "${params.deepvariant_output_dir}/vcf", mode: 'copy', overwrite: true, pattern: "*dv.vcf.gz*"
    cpus = 2
    maxRetries = 3
    memory = '8G'
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    disk '20 GB'
    time '700m'
    queue 'normal'
  }

  withName: gatk_haplotypecaller {
    container  = 'file:///software/hgi/containers/gatk_4.2.4.1.sif'
    containerOptions = "--bind /lustre --bind ${params.ref_dir}:/ref --bind ${params.bed_dir}:/bed_files --bind /tmp:/tmp"
    //clusterOptions = { "-M 8000 -R \"select[mem>=8000] rusage[mem=8000]\" -R \"select[model==Intel_Platinum]\"" }
    //publishDir "${params.haplotypecaller_output_dir}", mode: 'copy', overwrite: true, pattern: "*gatk.g.vcf.gz*"
    maxRetries = 3
    cpus = 2
    memory = '8G'
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    disk '20 GB'
    time '700m'
    queue 'normal'
  }

  withName: markDuplicates {
    container  = 'file:///software/hgi/containers/gatk_4.2.4.0.sif'
    containerOptions = "--bind /lustre --bind ${params.ref_dir}:/ref --bind ${params.bed_dir}:/bed_files --bind /tmp:/tmp"
    maxRetries = 3
    memory = '8G'
    cpus = 1
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    time '300m'
    queue 'normal'
  }

  withName: remap_cram {
    // container  = 'file:///software/hgi/containers/oqfe_remap.sif'
    // containerOptions = "--bind /lustre --bind /tmp --bind ${params.ref_dir}:/ref"
    cpus = 4
    maxRetries = 3
    stageInMode = 'copy'
    memory = '18G'
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    // errorStrategy 'terminate'
    //time '100m'
    //queue 'normal'
  }

  withName: sort_cram {
    container  = 'file:///software/hgi/containers/samtools_sambamba.sif'
    containerOptions = "--bind /lustre --bind /tmp --bind ${params.ref_dir}:/ref"
    maxRetries = 3
    cpus = 1
    memory = '18G'
    errorStrategy = { task.attempt <= 3 ? 'retry' : 'ignore' }
    // errorStrategy 'terminate'
    time '300m'
    queue 'normal'
  }
  cache = 'lenient'
  executor = 'lsf'
  shell = ['/bin/bash', '-euo', 'pipefail']
}

executor {
    name = 'lsf'
    queueSize = 4000
    poolSize = 4
    submitRateLimit = '10 sec'
    killBatchSize = 50
    pollInterval = '10 sec'
    queueStatInterval = '20 sec'
    dumpInterval = '10 sec'
    exitReadTimeout = '10 sec'
    perJobMemLimit = true
}

docker {
  enabled = false
}

singularity {
  enabled     = true
  autoMounts  = true
  cacheDir = '/software/hgi/containers/'
  runOptions = '--dns 172.18.255.1,172.18.255.2,172.18.255.3'
  envWhitelist = 'HOSTNAME,SSH_CONNECTION,SSH_CLIENT,CVS_RSH,http_proxy,https_proxy,HTTP_PROXY,HTTPS_PROXY'
}


profiles {
  lsf {
    includeConfig './nf_deepvariant/confs/lsf.conf'
    includeConfig './nf_deepvariant/confs/lsf_tasks.conf'
  }
}
